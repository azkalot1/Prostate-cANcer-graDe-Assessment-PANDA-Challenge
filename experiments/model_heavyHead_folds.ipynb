{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from panda_challenge import ClassifcationDatasetMultiCropOneImage\n",
    "from panda_challenge.train_utils import QWKCallback, get_optimizer, get_scheduler\n",
    "from panda_challenge import ClassifcationModel\n",
    "from panda_challenge.models import AdaptiveConcatPool2d\n",
    "from panda_challenge.utils import freeze, unfreeze\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
    "import torch\n",
    "from catalyst.contrib.nn.schedulers.onecycle import OneCycleLRWithWarmup\n",
    "from catalyst.contrib.nn.optimizers import RAdam, Lookahead\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.dl.callbacks import CriterionCallback\n",
    "from catalyst.core.callbacks import EarlyStoppingCallback\n",
    "from catalyst.core.callbacks import MetricAggregationCallback\n",
    "from catalyst.core.callbacks import CheckpointCallback\n",
    "\n",
    "from pytorch_toolbelt.losses import BinaryFocalLoss\n",
    "\n",
    "import os \n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "N_TILES = 36\n",
    "IMAGE_SIZE = 256\n",
    "LEVEL = 1\n",
    "BATCH_SIZE = 6\n",
    "NUM_WORKERS = 64\n",
    "N_EPOCHS = 100\n",
    "N_FROZEN_ENCODER = 7\n",
    "WARMUP_EPOCHS = 5\n",
    "INIT_LR = 3e-4\n",
    "WARMUP_FACTOR = 10\n",
    "NUM_CLASSES = 5\n",
    "IMAGE_FOLDER = '/data/personal_folders/skolchenko/panda/train_images/'\n",
    "N_TILES_ROW = int(np.sqrt(N_TILES))\n",
    "MODEL_NAME = 'resnet50'\n",
    "LOG_DIR = f'/data/personal_folders/skolchenko/panda/logs/{MODEL_NAME}_size{IMAGE_SIZE}_tiles{N_TILES}_heavyHead_fold{}'\n",
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train_individual = A.Compose([\n",
    "    A.OneOf(\n",
    "    [\n",
    "        A.Transpose(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.NoOp()\n",
    "    ], p=1.0),\n",
    "    A.OneOf(\n",
    "    [\n",
    "        A.ElasticTransform(p=1.0),\n",
    "        A.GridDistortion(p=1.0),\n",
    "        A.OpticalDistortion(p=1.0),\n",
    "        A.NoOp()\n",
    "    ], p=1.0),\n",
    "    A.OneOf(\n",
    "    [\n",
    "        A.GaussNoise(p=1.0),\n",
    "        A.GaussianBlur(p=1.0),\n",
    "        A.ISONoise(p=1.0),\n",
    "        A.CoarseDropout(p=1.0, max_holes=16, max_height=16, max_width=16),\n",
    "        A.NoOp()\n",
    "    ], p=1.0)\n",
    "])\n",
    "transforms_train_global = A.Compose([\n",
    "    A.Normalize(),\n",
    "    A.OneOf(\n",
    "    [\n",
    "        A.Transpose(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.NoOp()\n",
    "    ], p=1.0),\n",
    "    A.RandomGridShuffle(grid=(N_TILES_ROW, N_TILES_ROW))\n",
    "])\n",
    "transforms_train_global_tta = A.Compose([\n",
    "    A.OneOf(\n",
    "    [\n",
    "        A.Transpose(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.NoOp()\n",
    "    ], p=1.0),\n",
    "    A.RandomGridShuffle(grid=(N_TILES_ROW, N_TILES_ROW), p=1.0),\n",
    "    A.Normalize()\n",
    "])\n",
    "transforms_valid_global = A.Compose([\n",
    "    A.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into validation and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/personal_folders/skolchenko/panda/train_cleaned.csv')\n",
    "data_train, data_holdout = train_test_split(data, test_size=0.15, random_state=42, shuffle=True, stratify=data.isup_grade)\n",
    "data_train = data_train.reset_index(drop=True)\n",
    "data_holdout = data_holdout.reset_index(drop=True)\n",
    "data_train.loc[:, 'fold_idx'] = -1\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(data_train, data_train.isup_grade.values, groups=data_train.isup_grade.values)):\n",
    "    data_train.loc[test_index, 'fold_idx'] = fold_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold_idx):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = nn.MSELoss()\n",
    "    model = ClassifcationModel(model_name=MODEL_NAME, num_classes=NUM_CLASSES, pretrained=True)\n",
    "    model.head = nn.Sequential(\n",
    "                AdaptiveConcatPool2d((1, 1)),\n",
    "                nn.Flatten(),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(2*model.nc, model.nc//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.15),                \n",
    "                nn.Linear(model.nc//2, NUM_CLASSES))\n",
    "    model.cuda()    \n",
    "    \n",
    "    fold_data_train = data_train.loc[data_train['fold_idx']!=fold_idx]\n",
    "    fold_data_val = data_train.loc[data_train['fold_idx']==fold_idx]    \n",
    "    \n",
    "    train_dataset = ClassifcationDatasetMultiCropOneImage(\n",
    "        fold_data_train, \n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_FOLDER,\n",
    "        LEVEL,\n",
    "        N_TILES,\n",
    "        transform_individual=transforms_train_individual,\n",
    "        transform_global=transforms_train_global,\n",
    "        normalize=False,\n",
    "        load_pickled_tiles=True,\n",
    "        pickled_tiles_folder='/data/personal_folders/skolchenko/panda/pickled_tiled_images_{}_{}_{}'.format(LEVEL,\n",
    "                                                                                                            N_TILES, \n",
    "                                                                                                            IMAGE_SIZE),\n",
    "        output_type='ordinal'\n",
    "        )\n",
    "    val_dataset = ClassifcationDatasetMultiCropOneImage(\n",
    "        fold_data_val, \n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_FOLDER,\n",
    "        LEVEL,\n",
    "        N_TILES,\n",
    "        transform_global=transforms_valid_global,\n",
    "        normalize=False,\n",
    "        load_pickled_tiles=True,\n",
    "        pickled_tiles_folder='/data/personal_folders/skolchenko/panda/pickled_tiled_images_{}_{}_{}'.format(LEVEL,\n",
    "                                                                                                            N_TILES, \n",
    "                                                                                                            IMAGE_SIZE),\n",
    "        output_type='ordinal'\n",
    "        )\n",
    "    holdout_dataset = ClassifcationDatasetMultiCropOneImage(\n",
    "        data_holdout, \n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_FOLDER,\n",
    "        LEVEL,\n",
    "        N_TILES,\n",
    "        transform_global=transforms_valid_global,\n",
    "        normalize=False,\n",
    "        pickled_tiles_folder='/data/personal_folders/skolchenko/panda/pickled_tiled_images_{}_{}_{}'.format(LEVEL,\n",
    "                                                                                                            N_TILES, \n",
    "                                                                                                            IMAGE_SIZE),\n",
    "        load_pickled_tiles=True,       \n",
    "        output_type='ordinal'\n",
    "        )    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)  \n",
    "    holdout_loader = DataLoader(holdout_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)  \n",
    "    \n",
    "    loaders = collections.OrderedDict()\n",
    "    loaders[\"train\"] = train_loader\n",
    "    loaders[\"valid\"] = val_loader\n",
    "    loaders[\"holdout\"] = holdout_loader\n",
    "    losses = dict({\n",
    "        'loss_isup': criterion,\n",
    "    })\n",
    "    runner = SupervisedRunner(\n",
    "        input_key='features',\n",
    "        input_target_key=\"targets_isup\",\n",
    "        output_key=\"logits_isup\"\n",
    "        )    \n",
    "    \n",
    "    callbacks = [\n",
    "        CriterionCallback(\n",
    "            input_key=\"targets_isup\",\n",
    "            output_key=\"logits_isup\",\n",
    "            prefix=\"loss_isup\",\n",
    "            criterion_key='loss_isup',\n",
    "            multiplier=1.0\n",
    "        ),\n",
    "        QWKCallback(input_key=\"targets_isup\", \n",
    "                    output_key='logits_isup',\n",
    "                    qwk_name='ordinal'\n",
    "                    #qwk_name='simple',\n",
    "                   ),\n",
    "        MetricAggregationCallback(\n",
    "            prefix=\"loss\",\n",
    "            mode=\"weighted_sum\",\n",
    "            metrics={\n",
    "                \"loss_isup\": 1.0\n",
    "            }\n",
    "        ),\n",
    "        EarlyStoppingCallback(patience = PATIENCE, min_delta=1e-4),\n",
    "        CheckpointCallback(save_n_best = 5)    \n",
    "    ]    \n",
    "    \n",
    "    freeze(model)\n",
    "    optimizer = RAdam(model.parameters(), lr=INIT_LR)\n",
    "    optimizer = Lookahead(optimizer)\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=losses,\n",
    "        optimizer=optimizer,\n",
    "        callbacks=callbacks,\n",
    "        loaders=loaders,\n",
    "        logdir=LOG_DIR.format(fold_idx),\n",
    "        main_metric='loss',\n",
    "        num_epochs=N_FROZEN_ENCODER,\n",
    "        verbose=True,\n",
    "        minimize_metric=True,\n",
    "        fp16=True\n",
    "    )    \n",
    "    model.load_state_dict(torch.load(LOG_DIR.format(fold_idx)+'/checkpoints/best.pth')['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    unfreeze(model)\n",
    "    optimizer = RAdam(model.parameters(), lr=INIT_LR/WARMUP_FACTOR)\n",
    "    optimizer = Lookahead(optimizer)\n",
    "    scheduler_cosine = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        N_EPOCHS-WARMUP_EPOCHS)\n",
    "    scheduler = GradualWarmupScheduler(\n",
    "        optimizer,\n",
    "        multiplier=WARMUP_FACTOR, \n",
    "        total_epoch=WARMUP_EPOCHS,\n",
    "        after_scheduler=scheduler_cosine)    \n",
    "    \n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=losses,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        callbacks=callbacks,\n",
    "        loaders=loaders,\n",
    "        logdir=LOG_DIR.format(fold_idx),\n",
    "        main_metric='loss',\n",
    "        num_epochs=N_EPOCHS,\n",
    "        verbose=True,\n",
    "        minimize_metric=True,\n",
    "        fp16=True\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(state_dicts: List[dict]):\n",
    "    # source https://gist.github.com/qubvel/70c3d5e4cddcde731408f478e12ef87b\n",
    "    everage_dict = OrderedDict()\n",
    "    for k in state_dicts[0].keys():\n",
    "        everage_dict[k] = sum([state_dict[k] for state_dict in state_dicts]) / len(state_dicts)\n",
    "    return everage_dict\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    # source https://gist.github.com/qubvel/70c3d5e4cddcde731408f478e12ef87b\n",
    "    model.eval()\n",
    "    predicted_class = []\n",
    "    gt_class = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "            predictions = model(batch['features'].cuda())\n",
    "            predictions = nn.Sigmoid()(predictions)\n",
    "            predicted_class.extend(predictions.sum(dim=1).cpu().round().numpy())\n",
    "            gt_class.extend(batch['targets_isup'].sum(dim=1).cpu().numpy())\n",
    "    gt_class = np.array(gt_class).astype(int)\n",
    "    predicted_class = np.array(predicted_class).astype(int)\n",
    "    return(cohen_kappa_score(predicted_class, gt_class, weights='quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_epochs = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold_idx in range(5):\n",
    "for fold_idx in range(1):\n",
    "    print(f'====== Fold {fold_idx} =====')\n",
    "    model = ClassifcationModel(model_name=MODEL_NAME, num_classes=NUM_CLASSES, pretrained=True)\n",
    "    model.head = nn.Sequential(\n",
    "                AdaptiveConcatPool2d((1, 1)),\n",
    "                nn.Flatten(),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(2*model.nc, model.nc//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.15),                \n",
    "                nn.Linear(model.nc//2, NUM_CLASSES))\n",
    "    model.cuda()    \n",
    "    \n",
    "    fold_data_train = data_train.loc[data_train['fold_idx']!=fold_idx]\n",
    "    fold_data_val = data_train.loc[data_train['fold_idx']==fold_idx]    \n",
    "    \n",
    "    val_dataset = ClassifcationDatasetMultiCropOneImage(\n",
    "        fold_data_val, \n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_FOLDER,\n",
    "        LEVEL,\n",
    "        N_TILES,\n",
    "        transform_global=transforms_valid_global,\n",
    "        normalize=False,\n",
    "        #load_pickled_tiles=True,\n",
    "        #pickled_tiles_folder='/data/personal_folders/skolchenko/panda/pickled_tiled_images_{}_{}_{}'.format(LEVEL,\n",
    "        #                                                                                                    N_TILES, \n",
    "        #                                                                                                    IMAGE_SIZE)\n",
    "        )    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)  \n",
    "    \n",
    "    weights_path = [os.path.join(LOG_DIR.format(fold_idx), f'checkpoints/train.{epoch}.pth') for epoch in fold_epochs[fold_idx]]\n",
    "    all_weights = [torch.load(path)['model_state_dict'] for path in weights_path]\n",
    "\n",
    "    best_score = 0\n",
    "    best_weights = []\n",
    "\n",
    "    for w in all_weights:\n",
    "        current_weights = best_weights + [w]\n",
    "        average_dict = average_weights(current_weights)\n",
    "        model.load_state_dict(average_dict)\n",
    "        score = evaluate_model(model, val_loader)\n",
    "        print(f'Score: {score}')\n",
    "        if score > best_score:\n",
    "            print(f'New best score {score}')\n",
    "            best_score = score\n",
    "            best_weights.append(w)    \n",
    "            \n",
    "    best_dict = average_weights(best_weights)\n",
    "    model.load_state_dict(best_dict)\n",
    "    torch.save(model.state_dict(), os.path.join(LOG_DIR.format(fold_idx), f'checkpoints/averaged_best.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37564bitdeeplearningconda2f5dcc693383402099797ed40bd3951d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
